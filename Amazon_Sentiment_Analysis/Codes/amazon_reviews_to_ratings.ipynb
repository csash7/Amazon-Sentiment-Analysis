{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Libraries Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PANDAS, REGULAR EXPRESSION and NUMPY\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy\n",
    "\n",
    "#SKLEARN\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/siddharthmandgi/Desktop/Amazon_Reviews_to_Rating'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mAmazon_Datasets \u001b[m\u001b[m/                  amazon_reviews_to_Sentiment.ipynb\r\n",
      "DASK.ipynb                         amazon_reviews_to_ratings.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/siddharthmandgi/Desktop/Amazon_Reviews_to_Rating/Amazon_Datasets \n"
     ]
    }
   ],
   "source": [
    "cd '/Users/siddharthmandgi/Desktop/Amazon_Reviews_to_Rating/Amazon_Datasets /'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 3524: expected 15 fields, saw 22\\nSkipping line 5282: expected 15 fields, saw 22\\nSkipping line 20478: expected 15 fields, saw 22\\nSkipping line 25895: expected 15 fields, saw 22\\nSkipping line 27016: expected 15 fields, saw 22\\nSkipping line 59798: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 69198: expected 15 fields, saw 22\\nSkipping line 71953: expected 15 fields, saw 22\\nSkipping line 78720: expected 15 fields, saw 22\\nSkipping line 81300: expected 15 fields, saw 22\\nSkipping line 87683: expected 15 fields, saw 22\\nSkipping line 90516: expected 15 fields, saw 22\\nSkipping line 91147: expected 15 fields, saw 22\\nSkipping line 108945: expected 15 fields, saw 22\\nSkipping line 118289: expected 15 fields, saw 22\\nSkipping line 124667: expected 15 fields, saw 22\\nSkipping line 125541: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 144115: expected 15 fields, saw 22\\nSkipping line 167696: expected 15 fields, saw 22\\nSkipping line 179856: expected 15 fields, saw 22\\nSkipping line 189712: expected 15 fields, saw 22\\nSkipping line 196488: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 232132: expected 15 fields, saw 22\\nSkipping line 241734: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 269923: expected 15 fields, saw 22\\nSkipping line 279158: expected 15 fields, saw 22\\nSkipping line 283357: expected 15 fields, saw 22\\nSkipping line 285767: expected 15 fields, saw 22\\nSkipping line 308166: expected 15 fields, saw 22\\nSkipping line 315199: expected 15 fields, saw 22\\nSkipping line 315300: expected 15 fields, saw 22\\nSkipping line 316213: expected 15 fields, saw 22\\nSkipping line 317998: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 331459: expected 15 fields, saw 22\\nSkipping line 342509: expected 15 fields, saw 22\\nSkipping line 342825: expected 15 fields, saw 22\\nSkipping line 354794: expected 15 fields, saw 22\\nSkipping line 362229: expected 15 fields, saw 22\\nSkipping line 376919: expected 15 fields, saw 22\\nSkipping line 377584: expected 15 fields, saw 22\\nSkipping line 384728: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 404537: expected 15 fields, saw 22\\nSkipping line 407127: expected 15 fields, saw 22\\nSkipping line 407199: expected 15 fields, saw 22\\nSkipping line 407318: expected 15 fields, saw 22\\nSkipping line 417229: expected 15 fields, saw 22\\nSkipping line 417544: expected 15 fields, saw 22\\nSkipping line 435141: expected 15 fields, saw 22\\nSkipping line 445780: expected 15 fields, saw 22\\nSkipping line 446971: expected 15 fields, saw 22\\nSkipping line 456356: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 460217: expected 15 fields, saw 22\\nSkipping line 503770: expected 15 fields, saw 22\\nSkipping line 504599: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 543757: expected 15 fields, saw 22\\nSkipping line 546081: expected 15 fields, saw 22\\nSkipping line 547035: expected 15 fields, saw 22\\nSkipping line 550048: expected 15 fields, saw 22\\nSkipping line 550558: expected 15 fields, saw 22\\nSkipping line 562531: expected 15 fields, saw 22\\nSkipping line 563726: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 599564: expected 15 fields, saw 22\\nSkipping line 623644: expected 15 fields, saw 22\\nSkipping line 640089: expected 15 fields, saw 22\\nSkipping line 649887: expected 15 fields, saw 22\\nSkipping line 654117: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 657716: expected 15 fields, saw 22\\nSkipping line 659929: expected 15 fields, saw 22\\nSkipping line 674261: expected 15 fields, saw 22\\nSkipping line 679004: expected 15 fields, saw 22\\nSkipping line 683123: expected 15 fields, saw 22\\nSkipping line 692183: expected 15 fields, saw 22\\nSkipping line 700352: expected 15 fields, saw 22\\nSkipping line 701259: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 726436: expected 15 fields, saw 22\\nSkipping line 747628: expected 15 fields, saw 22\\nSkipping line 776068: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 799264: expected 15 fields, saw 22\\nSkipping line 806558: expected 15 fields, saw 22\\nSkipping line 815835: expected 15 fields, saw 22\\nSkipping line 825789: expected 15 fields, saw 22\\nSkipping line 832515: expected 15 fields, saw 22\\nSkipping line 833551: expected 15 fields, saw 22\\nSkipping line 843440: expected 15 fields, saw 22\\nSkipping line 843493: expected 15 fields, saw 22\\nSkipping line 843731: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 857694: expected 15 fields, saw 22\\nSkipping line 871364: expected 15 fields, saw 22\\nSkipping line 884645: expected 15 fields, saw 22\\nSkipping line 886120: expected 15 fields, saw 22\\nSkipping line 910878: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 932357: expected 15 fields, saw 22\\nSkipping line 933679: expected 15 fields, saw 22\\nSkipping line 934172: expected 15 fields, saw 22\\nSkipping line 935670: expected 15 fields, saw 22\\nSkipping line 939595: expected 15 fields, saw 22\\nSkipping line 967072: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 991557: expected 15 fields, saw 22\\nSkipping line 999316: expected 15 fields, saw 22\\nSkipping line 1000034: expected 15 fields, saw 22\\nSkipping line 1008871: expected 15 fields, saw 22\\nSkipping line 1018761: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1072693: expected 15 fields, saw 22\\nSkipping line 1105486: expected 15 fields, saw 22\\nSkipping line 1108163: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1115210: expected 15 fields, saw 22\\nSkipping line 1135407: expected 15 fields, saw 22\\nSkipping line 1138583: expected 15 fields, saw 22\\nSkipping line 1143289: expected 15 fields, saw 22\\nSkipping line 1145199: expected 15 fields, saw 22\\nSkipping line 1147208: expected 15 fields, saw 22\\nSkipping line 1162162: expected 15 fields, saw 22\\nSkipping line 1165191: expected 15 fields, saw 22\\nSkipping line 1167262: expected 15 fields, saw 22\\nSkipping line 1170422: expected 15 fields, saw 22\\nSkipping line 1175499: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1185352: expected 15 fields, saw 22\\nSkipping line 1190771: expected 15 fields, saw 22\\nSkipping line 1191562: expected 15 fields, saw 22\\nSkipping line 1200493: expected 15 fields, saw 22\\nSkipping line 1207009: expected 15 fields, saw 22\\nSkipping line 1210665: expected 15 fields, saw 22\\nSkipping line 1214281: expected 15 fields, saw 22\\nSkipping line 1224195: expected 15 fields, saw 22\\nSkipping line 1228210: expected 15 fields, saw 22\\nSkipping line 1234523: expected 15 fields, saw 22\\nSkipping line 1238400: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1254515: expected 15 fields, saw 22\\nSkipping line 1255457: expected 15 fields, saw 22\\nSkipping line 1273387: expected 15 fields, saw 22\\nSkipping line 1286172: expected 15 fields, saw 22\\nSkipping line 1292621: expected 15 fields, saw 22\\nSkipping line 1304301: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1315164: expected 15 fields, saw 22\\nSkipping line 1324128: expected 15 fields, saw 22\\nSkipping line 1353674: expected 15 fields, saw 22\\nSkipping line 1373124: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1387503: expected 15 fields, saw 22\\nSkipping line 1394693: expected 15 fields, saw 22\\nSkipping line 1403491: expected 15 fields, saw 22\\nSkipping line 1433326: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1496234: expected 15 fields, saw 22\\nSkipping line 1498327: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1510859: expected 15 fields, saw 22\\nSkipping line 1511601: expected 15 fields, saw 22\\nSkipping line 1528673: expected 15 fields, saw 22\\nSkipping line 1556136: expected 15 fields, saw 22\\nSkipping line 1563375: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1574281: expected 15 fields, saw 22\\nSkipping line 1594780: expected 15 fields, saw 22\\nSkipping line 1597538: expected 15 fields, saw 22\\nSkipping line 1610578: expected 15 fields, saw 22\\nSkipping line 1619131: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1639356: expected 15 fields, saw 22\\nSkipping line 1673602: expected 15 fields, saw 22\\nSkipping line 1679238: expected 15 fields, saw 22\\nSkipping line 1687574: expected 15 fields, saw 22\\nSkipping line 1691932: expected 15 fields, saw 22\\nSkipping line 1692750: expected 15 fields, saw 22\\nSkipping line 1702697: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1719706: expected 15 fields, saw 22\\nSkipping line 1727267: expected 15 fields, saw 22\\nSkipping line 1730634: expected 15 fields, saw 22\\nSkipping line 1732703: expected 15 fields, saw 22\\nSkipping line 1748259: expected 15 fields, saw 22\\nSkipping line 1754840: expected 15 fields, saw 22\\n'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 1771393: expected 15 fields, saw 22\\nSkipping line 1777950: expected 15 fields, saw 22\\nSkipping line 1780429: expected 15 fields, saw 22\\nSkipping line 1783894: expected 15 fields, saw 22\\nSkipping line 1815716: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1836542: expected 15 fields, saw 22\\nSkipping line 1837325: expected 15 fields, saw 22\\nSkipping line 1841712: expected 15 fields, saw 22\\nSkipping line 1846462: expected 15 fields, saw 22\\nSkipping line 1875025: expected 15 fields, saw 22\\nSkipping line 1885667: expected 15 fields, saw 22\\nSkipping line 1887273: expected 15 fields, saw 22\\nSkipping line 1894656: expected 15 fields, saw 22\\nSkipping line 1899233: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1924326: expected 15 fields, saw 22\\nSkipping line 1952474: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1968065: expected 15 fields, saw 22\\nSkipping line 1974616: expected 15 fields, saw 22\\nSkipping line 1975253: expected 15 fields, saw 22\\nSkipping line 1985530: expected 15 fields, saw 22\\nSkipping line 2017510: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2054246: expected 15 fields, saw 22\\nSkipping line 2059567: expected 15 fields, saw 22\\nSkipping line 2060366: expected 15 fields, saw 22\\nSkipping line 2084963: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2110060: expected 15 fields, saw 22\\nSkipping line 2141255: expected 15 fields, saw 22\\nSkipping line 2150314: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2174065: expected 15 fields, saw 22\\nSkipping line 2193567: expected 15 fields, saw 22\\nSkipping line 2194481: expected 15 fields, saw 22\\nSkipping line 2224645: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2235625: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2315134: expected 15 fields, saw 22\\nSkipping line 2356707: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2381237: expected 15 fields, saw 22\\nSkipping line 2408854: expected 15 fields, saw 22\\nSkipping line 2412478: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2453919: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2600523: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2637753: expected 15 fields, saw 22\\nSkipping line 2644735: expected 15 fields, saw 22\\nSkipping line 2660382: expected 15 fields, saw 22\\nSkipping line 2670926: expected 15 fields, saw 22\\nSkipping line 2678831: expected 15 fields, saw 22\\nSkipping line 2681185: expected 15 fields, saw 22\\nSkipping line 2685421: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2723748: expected 15 fields, saw 22\\nSkipping line 2741455: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2758664: expected 15 fields, saw 22\\nSkipping line 2762152: expected 15 fields, saw 22\\nSkipping line 2775984: expected 15 fields, saw 22\\nSkipping line 2787903: expected 15 fields, saw 22\\nSkipping line 2802792: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2830478: expected 15 fields, saw 22\\nSkipping line 2836932: expected 15 fields, saw 22\\nSkipping line 2847277: expected 15 fields, saw 22\\nSkipping line 2870724: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2929246: expected 15 fields, saw 22\\nSkipping line 2939718: expected 15 fields, saw 22\\nSkipping line 2944903: expected 15 fields, saw 22\\nSkipping line 2945398: expected 15 fields, saw 22\\nSkipping line 2946680: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2953410: expected 15 fields, saw 22\\nSkipping line 2957065: expected 15 fields, saw 22\\nSkipping line 3006213: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 3028333: expected 15 fields, saw 22\\nSkipping line 3042362: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 3106324: expected 15 fields, saw 22\\nSkipping line 3140250: expected 15 fields, saw 22\\nSkipping line 3141667: expected 15 fields, saw 22\\nSkipping line 3142917: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 3156696: expected 15 fields, saw 22\\nSkipping line 3159494: expected 15 fields, saw 22\\nSkipping line 3172832: expected 15 fields, saw 22\\nSkipping line 3190368: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 3237204: expected 15 fields, saw 22\\nSkipping line 3242114: expected 15 fields, saw 22\\nSkipping line 3243358: expected 15 fields, saw 22\\nSkipping line 3258982: expected 15 fields, saw 22\\nSkipping line 3261494: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 3285980: expected 15 fields, saw 22\\nSkipping line 3287556: expected 15 fields, saw 22\\nSkipping line 3297734: expected 15 fields, saw 22\\nSkipping line 3314897: expected 15 fields, saw 22\\nSkipping line 3319891: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 3346701: expected 15 fields, saw 22\\nSkipping line 3350661: expected 15 fields, saw 22\\nSkipping line 3353746: expected 15 fields, saw 22\\nSkipping line 3364196: expected 15 fields, saw 22\\nSkipping line 3383428: expected 15 fields, saw 22\\nSkipping line 3387465: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 3421118: expected 15 fields, saw 22\\nSkipping line 3423001: expected 15 fields, saw 22\\nSkipping line 3426328: expected 15 fields, saw 22\\nSkipping line 3427522: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 3477903: expected 15 fields, saw 22\\nSkipping line 3530035: expected 15 fields, saw 22\\nSkipping line 3530375: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 3542944: expected 15 fields, saw 22\\nSkipping line 3563598: expected 15 fields, saw 22\\nSkipping line 3582657: expected 15 fields, saw 22\\nSkipping line 3585525: expected 15 fields, saw 22\\nSkipping line 3592253: expected 15 fields, saw 22\\nSkipping line 3603659: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 3624426: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 3712450: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 3738960: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 3868638: expected 15 fields, saw 22\\nSkipping line 3906993: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 3932658: expected 15 fields, saw 22\\nSkipping line 3946211: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 4002121: expected 15 fields, saw 22\\nSkipping line 4037794: expected 15 fields, saw 22\\nSkipping line 4055051: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 4126261: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 4136235: expected 15 fields, saw 22\\nSkipping line 4186222: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 4206389: expected 15 fields, saw 22\\nSkipping line 4227217: expected 15 fields, saw 22\\nSkipping line 4246628: expected 15 fields, saw 22\\nSkipping line 4248387: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 4305738: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 4332985: expected 15 fields, saw 22\\nSkipping line 4342611: expected 15 fields, saw 22\\nSkipping line 4343614: expected 15 fields, saw 22\\nSkipping line 4360307: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 4415470: expected 15 fields, saw 22\\nSkipping line 4419078: expected 15 fields, saw 22\\nSkipping line 4446809: expected 15 fields, saw 22\\nSkipping line 4449653: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 4459302: expected 15 fields, saw 22\\nSkipping line 4477213: expected 15 fields, saw 22\\nSkipping line 4500863: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 4549886: expected 15 fields, saw 22\\nSkipping line 4552288: expected 15 fields, saw 22\\nSkipping line 4558859: expected 15 fields, saw 22\\nSkipping line 4562251: expected 15 fields, saw 22\\nSkipping line 4567914: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 4633361: expected 15 fields, saw 22\\nSkipping line 4640817: expected 15 fields, saw 22\\nSkipping line 4644917: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 4671853: expected 15 fields, saw 22\\nSkipping line 4701154: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 4721070: expected 15 fields, saw 22\\nSkipping line 4731356: expected 15 fields, saw 22\\nSkipping line 4744855: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 4939720: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 5184883: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 5279520: expected 15 fields, saw 22\\nSkipping line 5303010: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 5659632: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 5893492: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 5906173: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 6053734: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 6250800: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 6525510: expected 15 fields, saw 22\\n'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 6704025: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 7724708: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 8647349: expected 15 fields, saw 22\\n'\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "books = pd.read_table('/Users/siddharthmandgi/Desktop/Amazon_Reviews_to_Rating/Amazon_Datasets /amazon_reviews_us_Books_v1_00.tsv',error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10236850 entries, 0 to 10236849\n",
      "Data columns (total 15 columns):\n",
      "marketplace          object\n",
      "customer_id          int64\n",
      "review_id            object\n",
      "product_id           object\n",
      "product_parent       int64\n",
      "product_title        object\n",
      "product_category     object\n",
      "star_rating          object\n",
      "helpful_votes        float64\n",
      "total_votes          float64\n",
      "vine                 object\n",
      "verified_purchase    object\n",
      "review_headline      object\n",
      "review_body          object\n",
      "review_date          object\n",
      "dtypes: float64(2), int64(2), object(11)\n",
      "memory usage: 1.1+ GB\n"
     ]
    }
   ],
   "source": [
    "books.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['marketplace', 'customer_id', 'review_id', 'product_id',\n",
       "       'product_parent', 'product_title', 'product_category', 'star_rating',\n",
       "       'helpful_votes', 'total_votes', 'vine', 'verified_purchase',\n",
       "       'review_headline', 'review_body', 'review_date'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "marketplace             0\n",
       "customer_id             0\n",
       "review_id               0\n",
       "product_id              0\n",
       "product_parent          0\n",
       "product_title           0\n",
       "product_category        0\n",
       "star_rating            60\n",
       "helpful_votes          93\n",
       "total_votes            93\n",
       "vine                   93\n",
       "verified_purchase      93\n",
       "review_headline       163\n",
       "review_body           287\n",
       "review_date          1128\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = books.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10235459, 15)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5', '2', '4', '3', '1', 5, 1, 3, 4, 2]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = books['star_rating'].unique().tolist() #non uniform datatype\n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "books['star_rating'] = books['star_rating'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 2, 4, 3, 1]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = books['star_rating'].unique().tolist() #uniform datatype\n",
    "ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA With spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = list(STOP_WORDS) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "punctuations = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "parser = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenizer(sentence):\n",
    "    mytokens = parser(sentence)\n",
    "    mytokens = [word.lemma_.lower() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens]\n",
    "    mytokens = [word for word in mytokens if word not in stopwords and word not in punctuations]\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom Transformer using spaCy\n",
    "class predictors(TransformerMixin):\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        return [clean_text(text) for text in X]\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    def get_params(self,deep=True):\n",
    "        return{}\n",
    "    \n",
    "# Basic function to clean the text\n",
    "def clean_text(text):\n",
    "    return text.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdifVect = TfidfVectorizer(tokenizer = spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = books.head(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = data.drop('star_rating',axis=1)\n",
    "X = data['review_body']\n",
    "y = data['star_rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75232    My daughter loves these books!  We buy them al...\n",
       "48963    Jeff Shavitz’s book, “Size Doesn’t Matter: Why...\n",
       "44972                    old book, old info, in bad shape.\n",
       "13572                                            very good\n",
       "92740                                            thank you\n",
       "                               ...                        \n",
       "6267                                    abc in a cute way.\n",
       "54895    Cute idea of a story but the way the sentences...\n",
       "76832    Susan May Warren in her new book “Always on My...\n",
       "860      Wonderful, uplifting books for self and for gi...\n",
       "15800    Excellent thesis on Senator Warren's ideas and...\n",
       "Name: review_body, Length: 80000, dtype: object"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75232    5\n",
       "48963    5\n",
       "44972    2\n",
       "13572    5\n",
       "92740    5\n",
       "        ..\n",
       "6267     4\n",
       "54895    2\n",
       "76832    5\n",
       "860      5\n",
       "15800    5\n",
       "Name: star_rating, Length: 80000, dtype: int64"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MACHINE LEARNING ALGORITHMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "classifier = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('cleaner', predictors()),\n",
    "                 ('tfidfVect', tfdifVect),\n",
    "                ('classifier',classifier),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('cleaner', <__main__.predictors object at 0x137b2fe50>),\n",
       "                ('tfidfVect',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=N...\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<function spacy_tokenizer at 0x11da0a4d0>,\n",
       "                                 use_idf=True, vocabulary=None)),\n",
       "                ('classifier',\n",
       "                 LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                           fit_intercept=True, intercept_scaling=1,\n",
       "                           loss='squared_hinge', max_iter=1000,\n",
       "                           multi_class='ovr', penalty='l2', random_state=None,\n",
       "                           tol=0.0001, verbose=0))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_prediction = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1 )\n",
      "Here I am.  Now I am here.  I am.  Yes.  Enjoy being. 'PREDICTION': 5\n",
      "\n",
      " 2 )\n",
      "Great book, simply love it! It was a pleasure coloring, made me feel so much more relaxed. I highly recommend it and can't wait for the next one to be published. 'PREDICTION': 5\n",
      "\n",
      " 3 )\n",
      "Phenomenal read for anyone in transition! This book daily takes you through spiritual disciplines that help you stay focused on the fullest life in Christ and how to handle the post grad experience.  So thankful for Mrs. Robin and her words of encouragement.  Would recommend to the closest of friends! 'PREDICTION': 5\n",
      "\n",
      " 4 )\n",
      "very abstract... did nothing for me... waste of money do not buy!!!! no trading advise what so ever! 'PREDICTION': 1\n",
      "\n",
      " 5 )\n",
      "My review is strictly for The Hobbit & The Lord of the Rings Deluxe Pocket set.<br /><br />I love this set of books. Before I purchased, I took some time to read reviews on Amazon. Most reviewers mentioned that the font size was small. I have no issue with being able to read Tolkien's masterpiece. But, that is just me. As for quality, it is evident. The leather is soft and as leather should be. The box set comes in an embossed leather holding case which keeps the small books together. Other reviewers have mentioned that the colors of the covers are dull. I personally think that they are beyond beautiful. The colors are perfect for a set like this, from The Hobbit's purple syrah color to the copper of The Two Towers, to the army green color of The Return of the King. Another point that made me purchase this set over others was the size. You are able to carry these otherwise massive books with you anywhere. Believe me, I've been meaning to reread this series but purchased a huge edition with all three stories, sans The Hobbit, in one bindup. After getting past twelve pages I just couldn't do it anymore. This, however, is a set that is well worth your money. 'PREDICTION': 5\n",
      "\n",
      " 6 )\n",
      "Book is even better than the movie! 'PREDICTION': 5\n",
      "\n",
      " 7 )\n",
      "As a Notre Dame fan I really enjoyed  the perspective Mr. Pomarico provides of his time in South Bend. This book is more than just about Notre Dame football. It really is full of life lessons and I would encourage anyone who enjoys studying how others achieve success to read this. The motivational ideas inside the book can be applied to all areas of life. 'PREDICTION': 5\n",
      "\n",
      " 8 )\n",
      "its a must in your office for your daily work. 'PREDICTION': 5\n",
      "\n",
      " 9 )\n",
      "It is a good guide for people new to dating. 'PREDICTION': 5\n",
      "\n",
      " 10 )\n",
      "Having been an avid reader all my life, this book is probably the best I have ever read in this genre but also the most scary. It covers a comprehensive re-evaluation of US history over the last 150 years. The scary part is that a very different picture of the US as a world power emerges. The assessments offered are powerful and credible as every statement is backed up by references to declassified documents, interviews or correspondence. This book is a must read if there ever was one! 'PREDICTION': 5\n"
     ]
    }
   ],
   "source": [
    "count = 1\n",
    "for (sample,pred) in zip(X_test[0:10],sample_prediction[0:10]):\n",
    "    print('\\n',count,')')\n",
    "    print(sample,\"'PREDICTION':\", pred)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.74745\n"
     ]
    }
   ],
   "source": [
    "#Test Accuracy\n",
    "print(\"Accuracy:\", pipe.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.predict([\"I love this book\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.predict([\"Waste of Money!\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.predict([\"Fake! got stones instead!\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier =  LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('cleaner', predictors()),\n",
    "                ('tfidfVect', tfdifVect),\n",
    "                ('classifier',classifier)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('cleaner', <__main__.predictors object at 0x12f20ea90>),\n",
       "                ('tfidfVect',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=N...\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<function spacy_tokenizer at 0x11da0a4d0>,\n",
       "                                 use_idf=True, vocabulary=None)),\n",
       "                ('classifier',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='warn', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='warn', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_prediction = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7532\n"
     ]
    }
   ],
   "source": [
    "#Test Accuracy\n",
    "print(\"Accuracy:\", pipe.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('cleaner', predictors()),\n",
    "                ('tfidfVect', tfdifVect),\n",
    "                ('classifier',classifier)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('cleaner', <__main__.predictors object at 0x12d2d4a10>),\n",
       "                ('tfidfVect',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=N...\n",
       "                                 tokenizer=<function spacy_tokenizer at 0x11da0a4d0>,\n",
       "                                 use_idf=True, vocabulary=None)),\n",
       "                ('classifier',\n",
       "                 DecisionTreeClassifier(class_weight=None, criterion='gini',\n",
       "                                        max_depth=None, max_features=None,\n",
       "                                        max_leaf_nodes=None,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=1, min_samples_split=2,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        presort=False, random_state=None,\n",
       "                                        splitter='best'))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_prediction = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.68555\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", pipe.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENSEMBLE ALGORITHMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaggingClassifier(n_estimators=10, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('cleaner', predictors()),\n",
    "                ('tfidfVect', tfdifVect),\n",
    "                ('classifier', model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('cleaner', <__main__.predictors object at 0x11edfa210>),\n",
       "                ('tfidfVect',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=N...\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<function spacy_tokenizer at 0x127145cb0>,\n",
       "                                 use_idf=True, vocabulary=None)),\n",
       "                ('classifier',\n",
       "                 BaggingClassifier(base_estimator=None, bootstrap=True,\n",
       "                                   bootstrap_features=False, max_features=1.0,\n",
       "                                   max_samples=1.0, n_estimators=10,\n",
       "                                   n_jobs=None, oob_score=False, random_state=7,\n",
       "                                   verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.73\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", pipe.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AdaBoostClassifier(n_estimators=100, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('cleaner', predictors()),\n",
    "                ('tfidfVect', tfdifVect),\n",
    "                ('classifier', model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('cleaner', <__main__.predictors object at 0x11ef70bd0>),\n",
       "                ('tfidfVect',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<function spacy_tokenizer at 0x127145cb0>,\n",
       "                                 use_idf=True, vocabulary=None)),\n",
       "                ('classifier',\n",
       "                 AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "                                    learning_rate=1.0, n_estimators=100,\n",
       "                                    random_state=7))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7415\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", pipe.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(n_estimators = 100, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('cleaner', predictors()),\n",
    "                ('tfidfVect', tfdifVect),\n",
    "                ('classifier',classifier)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('cleaner', <__main__.predictors object at 0x12f50b710>),\n",
       "                ('tfidfVect',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=N...\n",
       "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                        criterion='gini', max_depth=None,\n",
       "                                        max_features='auto',\n",
       "                                        max_leaf_nodes=None,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=1, min_samples_split=2,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        n_estimators=100, n_jobs=None,\n",
       "                                        oob_score=False, random_state=42,\n",
       "                                        verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_prediction = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.74085\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", pipe.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Batch-Wise Predictions for our Big Data (1+ GB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using **LOGISTIC REGRESSION** for ensembling since it provided the highest accuracy overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data_list = np.array_split(books, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marketplace</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_category</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>vine</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>25933450</td>\n",
       "      <td>RJOVP071AVAJO</td>\n",
       "      <td>0439873800</td>\n",
       "      <td>84656342</td>\n",
       "      <td>There Was an Old Lady Who Swallowed a Shell!</td>\n",
       "      <td>Books</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>I love it and so does my students!</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "      <td>1801372</td>\n",
       "      <td>R1ORGBETCDW3AI</td>\n",
       "      <td>1623953553</td>\n",
       "      <td>729938122</td>\n",
       "      <td>I Saw a Friend</td>\n",
       "      <td>Books</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Please buy \"I Saw a Friend\"! Your children wil...</td>\n",
       "      <td>My wife and I ordered 2 books and gave them as...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>US</td>\n",
       "      <td>5782091</td>\n",
       "      <td>R7TNRFQAOUTX5</td>\n",
       "      <td>142151981X</td>\n",
       "      <td>678139048</td>\n",
       "      <td>Black Lagoon, Vol. 6</td>\n",
       "      <td>Books</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Shipped fast.</td>\n",
       "      <td>Great book just like all the others in the ser...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>US</td>\n",
       "      <td>32715830</td>\n",
       "      <td>R2GANXKDIFZ6OI</td>\n",
       "      <td>014241543X</td>\n",
       "      <td>712432151</td>\n",
       "      <td>If I Stay</td>\n",
       "      <td>Books</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>So beautiful</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>US</td>\n",
       "      <td>14005703</td>\n",
       "      <td>R2NYB6C3R8LVN6</td>\n",
       "      <td>1604600527</td>\n",
       "      <td>800572372</td>\n",
       "      <td>Stars 'N Strips Forever</td>\n",
       "      <td>Books</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Enjoyed the author's story and his quilts are ...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2047383</td>\n",
       "      <td>US</td>\n",
       "      <td>35585422</td>\n",
       "      <td>R39HG02Y8V7LWE</td>\n",
       "      <td>0310435307</td>\n",
       "      <td>260370434</td>\n",
       "      <td>NIV Giant Print Compact Bible</td>\n",
       "      <td>Books</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Excellent.</td>\n",
       "      <td>Perfect print size and easy to handle. Too bad...</td>\n",
       "      <td>2015-02-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2047384</td>\n",
       "      <td>US</td>\n",
       "      <td>8699723</td>\n",
       "      <td>R2X6DNUCOZV015</td>\n",
       "      <td>0979278031</td>\n",
       "      <td>313033861</td>\n",
       "      <td>Jetty Man</td>\n",
       "      <td>Books</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Great series of books based in our area</td>\n",
       "      <td>2015-02-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2047385</td>\n",
       "      <td>US</td>\n",
       "      <td>48359513</td>\n",
       "      <td>R3PZ4X28BR5289</td>\n",
       "      <td>1596435828</td>\n",
       "      <td>513092252</td>\n",
       "      <td>Giants Beware! (The Chronicles of Claudette)</td>\n",
       "      <td>Books</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Four Stars</td>\n",
       "      <td>Amazing book- great story with incredible illu...</td>\n",
       "      <td>2015-02-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2047386</td>\n",
       "      <td>US</td>\n",
       "      <td>7032112</td>\n",
       "      <td>R198Y7OVTU9IQ4</td>\n",
       "      <td>0764143573</td>\n",
       "      <td>455553794</td>\n",
       "      <td>Barron's Law Dictionary: Mass Market Edition (...</td>\n",
       "      <td>Books</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Great</td>\n",
       "      <td>2015-02-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2047387</td>\n",
       "      <td>US</td>\n",
       "      <td>52554709</td>\n",
       "      <td>R2NPJ1FTZ0E8RB</td>\n",
       "      <td>1416553649</td>\n",
       "      <td>335505464</td>\n",
       "      <td>Born Standing Up: A Comic's Life</td>\n",
       "      <td>Books</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Humorous and Insightful</td>\n",
       "      <td>It's fun read with both humor and insights int...</td>\n",
       "      <td>2015-02-24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2047092 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        marketplace  customer_id       review_id  product_id  product_parent  \\\n",
       "0                US     25933450   RJOVP071AVAJO  0439873800        84656342   \n",
       "1                US      1801372  R1ORGBETCDW3AI  1623953553       729938122   \n",
       "2                US      5782091   R7TNRFQAOUTX5  142151981X       678139048   \n",
       "3                US     32715830  R2GANXKDIFZ6OI  014241543X       712432151   \n",
       "4                US     14005703  R2NYB6C3R8LVN6  1604600527       800572372   \n",
       "...             ...          ...             ...         ...             ...   \n",
       "2047383          US     35585422  R39HG02Y8V7LWE  0310435307       260370434   \n",
       "2047384          US      8699723  R2X6DNUCOZV015  0979278031       313033861   \n",
       "2047385          US     48359513  R3PZ4X28BR5289  1596435828       513092252   \n",
       "2047386          US      7032112  R198Y7OVTU9IQ4  0764143573       455553794   \n",
       "2047387          US     52554709  R2NPJ1FTZ0E8RB  1416553649       335505464   \n",
       "\n",
       "                                             product_title product_category  \\\n",
       "0             There Was an Old Lady Who Swallowed a Shell!            Books   \n",
       "1                                           I Saw a Friend            Books   \n",
       "2                                     Black Lagoon, Vol. 6            Books   \n",
       "3                                                If I Stay            Books   \n",
       "4                                  Stars 'N Strips Forever            Books   \n",
       "...                                                    ...              ...   \n",
       "2047383                      NIV Giant Print Compact Bible            Books   \n",
       "2047384                                          Jetty Man            Books   \n",
       "2047385       Giants Beware! (The Chronicles of Claudette)            Books   \n",
       "2047386  Barron's Law Dictionary: Mass Market Edition (...            Books   \n",
       "2047387                   Born Standing Up: A Comic's Life            Books   \n",
       "\n",
       "         star_rating  helpful_votes  total_votes vine verified_purchase  \\\n",
       "0                  5            0.0          0.0    N                 Y   \n",
       "1                  5            0.0          0.0    N                 Y   \n",
       "2                  5            0.0          0.0    N                 Y   \n",
       "3                  5            0.0          0.0    N                 N   \n",
       "4                  5            2.0          2.0    N                 Y   \n",
       "...              ...            ...          ...  ...               ...   \n",
       "2047383            5            0.0          1.0    N                 Y   \n",
       "2047384            5            0.0          0.0    N                 Y   \n",
       "2047385            4            1.0          1.0    N                 N   \n",
       "2047386            5            0.0          0.0    N                 Y   \n",
       "2047387            5            0.0          0.0    N                 Y   \n",
       "\n",
       "                                           review_headline  \\\n",
       "0                                               Five Stars   \n",
       "1        Please buy \"I Saw a Friend\"! Your children wil...   \n",
       "2                                            Shipped fast.   \n",
       "3                                               Five Stars   \n",
       "4                                               Five Stars   \n",
       "...                                                    ...   \n",
       "2047383                                         Excellent.   \n",
       "2047384                                         Five Stars   \n",
       "2047385                                         Four Stars   \n",
       "2047386                                         Five Stars   \n",
       "2047387                            Humorous and Insightful   \n",
       "\n",
       "                                               review_body review_date  \n",
       "0                       I love it and so does my students!  2015-08-31  \n",
       "1        My wife and I ordered 2 books and gave them as...  2015-08-31  \n",
       "2        Great book just like all the others in the ser...  2015-08-31  \n",
       "3                                             So beautiful  2015-08-31  \n",
       "4        Enjoyed the author's story and his quilts are ...  2015-08-31  \n",
       "...                                                    ...         ...  \n",
       "2047383  Perfect print size and easy to handle. Too bad...  2015-02-24  \n",
       "2047384            Great series of books based in our area  2015-02-24  \n",
       "2047385  Amazing book- great story with incredible illu...  2015-02-24  \n",
       "2047386                                              Great  2015-02-24  \n",
       "2047387  It's fun read with both humor and insights int...  2015-02-24  \n",
       "\n",
       "[2047092 rows x 15 columns]"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "sample_predictions_list_final = []\n",
    "loaded_models_final = []\n",
    "for i in data_list:\n",
    "    X = i['review_body']\n",
    "    y = i['star_rating']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2, random_state=42)\n",
    "    classifier =  LogisticRegression(multi_class='auto')\n",
    "    pipe = Pipeline([('cleaner', predictors()),\n",
    "                     ('tfidfVect', tfdifVect),\n",
    "                     ('classifier',classifier)])\n",
    "    pipe.fit(X_train,y_train)\n",
    "    filename = str(count) + 'finalized_model.sav'\n",
    "    pickle.dump(pipe, open(filename, 'wb'))\n",
    "    loaded_models_final.append(pickle.load(open(filename, 'rb')))\n",
    "    sample_predictions = loaded_models_final[count-1].predict(X_test)\n",
    "    sample_predictions_list_final.append(sample_predictions)\n",
    "    count += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Pipeline(memory=None,\n",
       "          steps=[('cleaner', <__main__.predictors object at 0x3a9868490>),\n",
       "                 ('vectorizer',\n",
       "                  CountVectorizer(analyzer='word', binary=False,\n",
       "                                  decode_error='strict',\n",
       "                                  dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                  input='content', lowercase=True, max_df=1.0,\n",
       "                                  max_features=None, min_df=1,\n",
       "                                  ngram_range=(1, 1), preprocessor=None,\n",
       "                                  stop_words=None, strip_accents=None,\n",
       "                                  toke...?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                  tokenizer=<function spacy_tokenizer at 0x122bd6170>,\n",
       "                                  vocabulary=None)),\n",
       "                 ('classifier',\n",
       "                  LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                     fit_intercept=True, intercept_scaling=1,\n",
       "                                     l1_ratio=None, max_iter=100,\n",
       "                                     multi_class='warn', n_jobs=None,\n",
       "                                     penalty='l2', random_state=None,\n",
       "                                     solver='warn', tol=0.0001, verbose=0,\n",
       "                                     warm_start=False))],\n",
       "          verbose=False), Pipeline(memory=None,\n",
       "          steps=[('cleaner', <__main__.predictors object at 0x3c73a4950>),\n",
       "                 ('vectorizer',\n",
       "                  CountVectorizer(analyzer='word', binary=False,\n",
       "                                  decode_error='strict',\n",
       "                                  dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                  input='content', lowercase=True, max_df=1.0,\n",
       "                                  max_features=None, min_df=1,\n",
       "                                  ngram_range=(1, 1), preprocessor=None,\n",
       "                                  stop_words=None, strip_accents=None,\n",
       "                                  toke...?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                  tokenizer=<function spacy_tokenizer at 0x122bd6170>,\n",
       "                                  vocabulary=None)),\n",
       "                 ('classifier',\n",
       "                  LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                     fit_intercept=True, intercept_scaling=1,\n",
       "                                     l1_ratio=None, max_iter=100,\n",
       "                                     multi_class='warn', n_jobs=None,\n",
       "                                     penalty='l2', random_state=None,\n",
       "                                     solver='warn', tol=0.0001, verbose=0,\n",
       "                                     warm_start=False))],\n",
       "          verbose=False), Pipeline(memory=None,\n",
       "          steps=[('cleaner', <__main__.predictors object at 0x384011690>),\n",
       "                 ('vectorizer',\n",
       "                  CountVectorizer(analyzer='word', binary=False,\n",
       "                                  decode_error='strict',\n",
       "                                  dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                  input='content', lowercase=True, max_df=1.0,\n",
       "                                  max_features=None, min_df=1,\n",
       "                                  ngram_range=(1, 1), preprocessor=None,\n",
       "                                  stop_words=None, strip_accents=None,\n",
       "                                  toke...?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                  tokenizer=<function spacy_tokenizer at 0x122bd6170>,\n",
       "                                  vocabulary=None)),\n",
       "                 ('classifier',\n",
       "                  LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                     fit_intercept=True, intercept_scaling=1,\n",
       "                                     l1_ratio=None, max_iter=100,\n",
       "                                     multi_class='warn', n_jobs=None,\n",
       "                                     penalty='l2', random_state=None,\n",
       "                                     solver='warn', tol=0.0001, verbose=0,\n",
       "                                     warm_start=False))],\n",
       "          verbose=False), Pipeline(memory=None,\n",
       "          steps=[('cleaner', <__main__.predictors object at 0x39328e490>),\n",
       "                 ('vectorizer',\n",
       "                  CountVectorizer(analyzer='word', binary=False,\n",
       "                                  decode_error='strict',\n",
       "                                  dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                  input='content', lowercase=True, max_df=1.0,\n",
       "                                  max_features=None, min_df=1,\n",
       "                                  ngram_range=(1, 1), preprocessor=None,\n",
       "                                  stop_words=None, strip_accents=None,\n",
       "                                  toke...?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                  tokenizer=<function spacy_tokenizer at 0x122bd6170>,\n",
       "                                  vocabulary=None)),\n",
       "                 ('classifier',\n",
       "                  LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                     fit_intercept=True, intercept_scaling=1,\n",
       "                                     l1_ratio=None, max_iter=100,\n",
       "                                     multi_class='warn', n_jobs=None,\n",
       "                                     penalty='l2', random_state=None,\n",
       "                                     solver='warn', tol=0.0001, verbose=0,\n",
       "                                     warm_start=False))],\n",
       "          verbose=False), Pipeline(memory=None,\n",
       "          steps=[('cleaner', <__main__.predictors object at 0x33503b650>),\n",
       "                 ('vectorizer',\n",
       "                  CountVectorizer(analyzer='word', binary=False,\n",
       "                                  decode_error='strict',\n",
       "                                  dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                  input='content', lowercase=True, max_df=1.0,\n",
       "                                  max_features=None, min_df=1,\n",
       "                                  ngram_range=(1, 1), preprocessor=None,\n",
       "                                  stop_words=None, strip_accents=None,\n",
       "                                  toke...?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                  tokenizer=<function spacy_tokenizer at 0x122bd6170>,\n",
       "                                  vocabulary=None)),\n",
       "                 ('classifier',\n",
       "                  LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                     fit_intercept=True, intercept_scaling=1,\n",
       "                                     l1_ratio=None, max_iter=100,\n",
       "                                     multi_class='warn', n_jobs=None,\n",
       "                                     penalty='l2', random_state=None,\n",
       "                                     solver='warn', tol=0.0001, verbose=0,\n",
       "                                     warm_start=False))],\n",
       "          verbose=False)]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_models_final #All 5 Models for all 5 batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([5, 5, 5, ..., 5, 5, 3]),\n",
       " array([5, 5, 5, ..., 5, 5, 5]),\n",
       " array([5, 3, 5, ..., 4, 5, 5]),\n",
       " array([5, 5, 5, ..., 5, 5, 5]),\n",
       " array([5, 5, 5, ..., 5, 5, 1])]"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_predictions_list_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predictions_DataFrame = pd.DataFrame({'Batch1': sample_predictions_list_final[0],\n",
    "                                      'Batch2': sample_predictions_list_final[1],\n",
    "                                      'Batch3': sample_predictions_list_final[2],\n",
    "                                      'Batch4': sample_predictions_list_final[3],\n",
    "                                      'Batch5': sample_predictions_list_final[4]},index=Fals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Batch1</th>\n",
       "      <th>Batch2</th>\n",
       "      <th>Batch3</th>\n",
       "      <th>Batch4</th>\n",
       "      <th>Batch5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>409414</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>409415</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>409416</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>409417</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>409418</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>409419 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Batch1  Batch2  Batch3  Batch4  Batch5\n",
       "0            5       5       5       5       5\n",
       "1            5       5       3       5       5\n",
       "2            5       5       5       5       5\n",
       "3            5       5       5       5       4\n",
       "4            5       5       5       5       4\n",
       "...        ...     ...     ...     ...     ...\n",
       "409414       5       5       5       5       5\n",
       "409415       5       5       5       5       5\n",
       "409416       5       5       4       5       5\n",
       "409417       5       5       5       5       5\n",
       "409418       3       5       5       5       1\n",
       "\n",
       "[409419 rows x 5 columns]"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Predictions_DataFrame.to_csv('Predictions_DataFrame') # Ratings match the size of X_test\n",
    "Predictions_DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 3, 4, 1, 2]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = Predictions_DataFrame['Batch2'].unique().tolist() #confirmation for those batches which seem to \n",
    "                                                            #have l unique value only\n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 2, 3, 4, 1]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = Predictions_DataFrame['Batch4'].unique().tolist() #confirmation for those batches which seem to \n",
    "                                                            #have l unique value only\n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5. , 4.6, 5. , ..., 4.8, 5. , 3.8])"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_predictions = sum(sample_predictions_list_final)/len(sample_predictions_list_final)\n",
    "mean_predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
